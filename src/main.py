import os
from contextlib import asynccontextmanager
from enum import Enum

import autogen.io
from autogen import Agent, AssistantAgent, UserProxyAgent, register_function, ConversableAgent, GroupChat, \
    GroupChatManager
from autogen.coding import DockerCommandLineCodeExecutor
from dotenv import load_dotenv, find_dotenv
import openai

from fastapi import FastAPI
from autogen.io.base import IOStream
from autogen.io.websockets import IOWebsockets
from agents.group_chat import CodeGenGroupChat
from rich.console import Console
from rich.text import Text
from typing import Any

from agents.tools.github_tools import create_github_gist
from agents.utils import render_template
from llm_config import GPT4Turbo_config

load_dotenv(find_dotenv())
openai.api_key = os.environ['OPENAI_API_KEY']
openai.log='debug'

PORT = 8080
app = FastAPI()


class RichIOStream(IOStream):
    def __init__(self):
        self.console = Console()

    def print(self, *args: Any, **kwargs) -> None:
        # Remove 'flush' argument if present, as it's not supported by rich.console.Console.print
        kwargs.pop('flush', None)
        processed_args = []
        for arg in args:
            if isinstance(arg, str):
                # Convert args with ANSI codes into rich Text objects
                processed_args.append(Text.from_ansi(arg))
            else:
                # Non-string arguments are added without modification
                processed_args.append(arg)

        self.console.print(markup=False, *processed_args, **kwargs)

    def input(self, prompt: str = "", *, password: bool = False) -> str:
        return input(prompt)


def set_custom_io_overrides():
    autogen.io.IOStream.set_global_default(RichIOStream())
    autogen.io.IOStream.set_default(RichIOStream())


def on_connect(iostream: IOWebsockets) -> None:
    # Set global default stream
    set_custom_io_overrides()
    print(f" - on_connect(): Connected to client using IOWebsockets {iostream}", flush=True)
    print(" - on_connect(): Receiving message from client.", flush=True)

    # Receive Initial Message
    initial_msg = iostream.input()

    # Instantiate GroupChat
    group_chat = CodeGenGroupChat()

    # Register the tool function for use in the workflow
    register_function(
        create_github_gist,
        caller=group_chat.check_in,
        executor=group_chat.user_proxy,
        name="create_github_gist",
        description="Create a GitHub Gist from the code generated by the programmer.",
    )

    # Start conversation
    print(
        f" - on_connect(): Initiating group-chat with {group_chat} using message '{initial_msg}'",
        flush=True,
    )
    group_chat.start(message=initial_msg)


@asynccontextmanager
async def run_websocket_server(app):
    with IOWebsockets.run_server_in_thread(on_connect=on_connect, port=PORT) as uri:
        print(f"Websocket server started at {uri}.", flush=True)

        yield

app = FastAPI(lifespan=run_websocket_server)


class OpenAIModels(Enum):
    GPT4 = "gpt-4"
    GPT4_TURBO = "gpt-4-turbo-preview"
    GPT3_5_TURBO = "gpt-3.5-turbo"



@app.get("/")
async def get():
    return {"message": "AutoGen Group-chat"}

